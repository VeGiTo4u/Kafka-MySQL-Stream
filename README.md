# Kafka to MySQL Stream

Welcome to the Kafka to MySQL Stream Project repository! ğŸš€
This project demonstrates a real-time streaming pipeline from MySQL â†’ Kafka â†’ JSON outputs, with incremental loading and parallel consumers. Designed as a portfolio project, it highlights practical data engineering concepts like streaming ingestion, consumer parallelism, and duplicate prevention.

â¸»

# ğŸ—ï¸ **Data Architecture**

**The architecture follows a Producerâ€“Brokerâ€“Consumer flow:**

1.	MySQL (Source): Data is loaded into MySQL tables.
2.	Kafka Producer: Reads MySQL data and publishes only incremental records based on the last-read timestamp.
3.	Kafka Broker: Manages the published data streams.
4.	Kafka Consumers (5 Instances): Consume messages in parallel and write separate JSON files (consumer1.json, consumer2.json, etc.).

# **ğŸ“– Project Overview**

**This project involves:**
	1.	Incremental Data Ingestion: Load new records from MySQL into Kafka without duplicates.
	2.	Streaming Pipeline: Leverage Kafka for distributing messages across multiple consumers.
	3.	Parallel Consumers: Run 5 consumer instances under a consumer group, each writing to its own JSON file.
	4.	Scalable Architecture: The consumer group can be expanded or reduced depending on workload.

	â€¢	Apache Kafka
	â€¢	Real-time Data Streaming
	â€¢	MySQL Data Integration
	â€¢	Parallel Processing with Consumer Groups
	â€¢	JSON-based Data Outputs

â¸»

# **Tech Stack** 
	â€¢	Apache Kafka â€“ Distributed message streaming platform
	â€¢	MySQL/MySQL Workbench â€“ Relational database for source data
	â€¢	Python / Java â€“ For Kafka producer & consumers
	â€¢	JSON â€“ Output format for processed messages

â¸»

# **Project Workflow**

**Objective - Build a real-time pipeline that streams MySQL data to Kafka and outputs JSON files through multiple consumers with incremental loading.**

Workflow Steps
	1.	Load Data into MySQL â€“ Insert records into MySQL database tables.
	2.	Producer â€“ Reads new records (greater than last timestamp) and publishes them into a Kafka topic.
	3.	Kafka Broker â€“ Distributes data to consumers.
	4.	Consumers â€“ 5 parallel instances consume the messages and each writes its own JSON file.
	5.	Output â€“ Clean, duplicate-free JSON files per consumer instance.

â¸»

# **ğŸ“‚ Repository Structure**

â€¢ kafka-to-mysql-stream/
	â”‚
	â”œâ”€â”€ producer/             # Producer code (MySQL â†’ Kafka)
	â”‚
	â”œâ”€â”€ consumers/            # Consumer code (Kafka â†’ JSON)
	â”‚
	â”œâ”€â”€ config/               # Config files for Kafka and MySQL
	â”‚
	â”œâ”€â”€ sample_data/          # Example MySQL datasets
	â”‚
	â”œâ”€â”€ output/               # JSON outputs from consumers
	â”‚
	â”œâ”€â”€ docs/                 # Documentation & architecture diagrams
	â”‚   â”œâ”€â”€ kafka_mysql_architecture.png
	â”‚   â””â”€â”€ design_notes.md
	â”‚
	â”œâ”€â”€ README.md             # Project overview and instructions
	â”œâ”€â”€ LICENSE               # License information
	â””â”€â”€ .gitignore            # Git ignore file

# **Key Learning Outcomes**
	â€¢	Setting up a Kafka producer connected to MySQL
	â€¢	Using timestamps for incremental loading and duplicate prevention
	â€¢	Understanding Kafka consumer groups and partition assignment
	â€¢	Implementing parallel consumers writing to JSON files

â¸»

# **Future Improvements**
	â€¢	ğŸ³ Add Docker setup for seamless environment configuration
	â€¢	ğŸ“‘ Implement schema validation for JSON output files
	â€¢	ğŸ“Š Extend with real-time dashboards using Kafka + Spark/Flink
	â€¢	ğŸ—„ï¸ Push consumer JSON outputs into a NoSQL store like MongoDB

â¸»

# **ğŸ›¡ï¸ License**

This project is licensed under the MIT License. You are free to use, modify, and share this project with proper attribution.

â¸»

# **ğŸŒŸ About Me**

Hi there! Iâ€™m Krrish Sethiya. Iâ€™m a 3rd Year Grad at Medicaps University, Indore, currently upskilling myself in Data Engineering and Streaming Technologies.
