# Kafka to MySQL Stream

Welcome to the Kafka to MySQL Stream Project repository! ğŸš€
This project demonstrates a real-time streaming pipeline from MySQL â†’ Kafka â†’ JSON outputs, with incremental loading and parallel consumers. Designed as a portfolio project, it highlights practical data engineering concepts like streaming ingestion, consumer parallelism, and duplicate prevention.

â¸»

# ğŸ—ï¸ **Data Architecture**

**The architecture follows a Producerâ€“Brokerâ€“Consumer flow:**

1.	MySQL (Source): Data is loaded into MySQL tables.
2.	Kafka Producer: Reads MySQL data and publishes only incremental records based on the last-read timestamp.
3.	Kafka Broker: Manages the published data streams.
4.	Kafka Consumers (5 Instances): Consume messages in parallel and write separate JSON files (consumer1.json, consumer2.json, etc.).

# **ğŸ“– Project Overview**

**This project involves:**
	1.	Incremental Data Ingestion: Load new records from MySQL into Kafka without duplicates.
	2.	Streaming Pipeline: Leverage Kafka for distributing messages across multiple consumers.
	3.	Parallel Consumers: Run 5 consumer instances under a consumer group, each writing to its own JSON file.
	4.	Scalable Architecture: The consumer group can be expanded or reduced depending on workload.

	â€¢	Apache Kafka
	â€¢	Real-time Data Streaming
	â€¢	MySQL Data Integration
	â€¢	Parallel Processing with Consumer Groups
	â€¢	JSON-based Data Outputs

â¸»

# **Tech Stack** 
	â€¢	Apache Kafka â€“ Distributed message streaming platform
	â€¢	MySQL/MySQL Workbench â€“ Relational database for source data
	â€¢	Python / Java â€“ For Kafka producer & consumers
	â€¢	JSON â€“ Output format for processed messages

â¸»

# **Project Workflow**

**Objective - Build a real-time pipeline that streams MySQL data to Kafka and outputs JSON files through multiple consumers with incremental loading.**

Workflow Steps

	1.	Load Data into MySQL â€“ Insert records into MySQL database tables.
	2.	Producer â€“ Reads new records (greater than last timestamp) and publishes them into a Kafka topic.
	3.	Kafka Broker â€“ Distributes data to consumers.
	4.	Consumers â€“ 5 parallel instances consume the messages and each writes its own JSON file.
	5.	Output â€“ Clean, duplicate-free JSON files per consumer instance.

â¸»

# **ğŸ“‚ Repository Structure**

```
kafka-mysql-stream/
|
â”œâ”€â”€ Producer-Cosumer/
|       â”œâ”€â”€ consumer.py        
â”‚   	â””â”€â”€ producer.py

â”œâ”€â”€ Avro Schema
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ LICENSE                 # License information
â”œâ”€â”€ System Guide
```

# **Key Learning Outcomes**
	â€¢	Setting up a Kafka producer connected to MySQL
	â€¢	Using timestamps for incremental loading and duplicate prevention
	â€¢	Understanding Kafka consumer groups and partition assignment
	â€¢	Implementing parallel consumers writing to JSON files

â¸»

# **Future Improvements**
	â€¢	ğŸ³ Add Docker setup for seamless environment configuration
	â€¢	ğŸ“‘ Implement schema validation for JSON output files
	â€¢	ğŸ“Š Extend with real-time dashboards using Kafka + Spark/Flink
	â€¢	ğŸ—„ï¸ Push consumer JSON outputs into a NoSQL store like MongoDB

â¸»

# **ğŸ›¡ï¸ License**

This project is licensed under the MIT License. You are free to use, modify, and share this project with proper attribution.

â¸»

# **ğŸŒŸ About Me**

Hi there! Iâ€™m Krrish Sethiya. Iâ€™m a 3rd Year Grad at Medicaps University, Indore, currently upskilling myself in Data Engineering and Streaming Technologies.
